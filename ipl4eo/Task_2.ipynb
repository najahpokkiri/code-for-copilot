{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74f2ef0d-8fc7-40b5-9aa4-5babd093f365",
   "metadata": {},
   "source": [
    "# Task 2: CNN Training for pixel-wise classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd45dd54-dcd0-4446-8eae-468280ee243d",
   "metadata": {},
   "source": [
    "In this task you will be provided a model that was pretrained on [BigEarthNet v2](https://arxiv.org/abs/1902.06148) for pixel-wise classification (i.e. semantic segmentation). We will provide you with a checkpoint, as well as the model definition and your task is to load that model using these weights and finetune it on our target domain (forest segmentation) in our target location (Amazon Rainforest) with pytorch lightning. For that we will provide you with a finetuning dataset.\n",
    "\n",
    "<img src=\"../../../data/Example_finetune.png\" alt=\"Example from Dataset\" width=\"600\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e960f3-6a04-4668-9419-868a1577b76e",
   "metadata": {},
   "source": [
    "The goals of this task are as follows:\n",
    "1. Load a pretrained pixelwise segmentation model\n",
    "2. Adapt and finetune the model on a new domain (forest segmentation) and location (Amazon Rain Forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35b29d9-76b8-419a-88e8-2852d169e98a",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "These are all imports we used when solving the task. Please leave them as is even though you might not need all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29eb635d-ec58-4b38-8ca8-72cac53fddd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import rootutils\n",
    "root = rootutils.setup_root(os.path.abspath(''), dotenv=True, pythonpath=True, cwd=False)\n",
    "\n",
    "data_path = root / \"data\"\n",
    "data_path.mkdir(exist_ok=True)\n",
    "output_dir = root / \"output\"\n",
    "output_dir.mkdir(exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69b81d6-6583-427d-bfbd-6b1b09b27aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from types import SimpleNamespace\n",
    "from huggingface_hub import PyTorchModelHubMixin\n",
    "import lightning as L\n",
    "from configilm import ConfigILM # see https://lhackel-tub.github.io/ConfigILM/ for more information\n",
    "from torchinfo import summary\n",
    "\n",
    "from torchmetrics.segmentation import MeanIoU\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import lmdb\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from safetensors.numpy import load as load_np_safetensor\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib.patches import Patch\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376d4556-dfd4-41a1-a1b7-571c08fee0dd",
   "metadata": {},
   "source": [
    "## 2.1 Dataset + DataModule definition\n",
    "\n",
    "Before we can use our data we need to wrap it in a pytorch dataset and thereafter in a lightning DataModule so we can use it for model training. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150d8546-062e-4e46-b89e-33c3f4a036f3",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "For efficient data loading we have put the images in the file `images.lmdb` and the segmentation masks (forest/ no forest) in the file `mask.lmdb`. [LMDB](http://www.lmdb.tech/doc/) is a key-value in-memory database. For the images the key is the image name (1.tif, 2.tif,...) and the values are the image pixels as safetensor (Tip: use `load_np_safetensor` to read it). For the masks the key is the image name followed by _mask (1_mask.tif, 2_mask.tif, ...) the value again is the pixels as safetensor (1 for forest, 0 for no forest). We provided the helper function `_open_lmdb` which opens a connection to the lmdb for images or masks if it does not exist yet. You can read data from the lmdb through `with self.env_images.begin() as txn: txn.get()`. Feel free to add additional functions and adapt the already existing ones. Please open the lmdb only in the `__getitem__` method, due to multi processing.\n",
    "Use preprocessing and data augmentation where applicable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabf9ceb-3a0d-4b23-90f1-e2acd3092a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean =  [438.37207031, 614.05566406, 588.40960693, 2193.29199219, 942.84332275, 1769.93164062, 2049.55151367, 1568.22680664, 997.73248291, 2235.55664062]\n",
    "std = [607.02685547, 603.29681396, 684.56884766, 1369.3717041, 738.43267822, 1100.45605469, 1275.80541992, 1070.16125488, 813.52764893, 1356.54406738]\n",
    "\n",
    "\n",
    "class FinetuneDataset(Dataset):\n",
    "    def __init__(self, images_lmdb_path=data_path / \"images.lmdb\", masks_lmdb_path=data_path / \"mask.lmdb\", transform=None):\n",
    "        self.images_lmdb_path = images_lmdb_path\n",
    "        self.masks_lmdb_path = masks_lmdb_path\n",
    "\n",
    "        self.env_images = None\n",
    "        self.env_masks = None\n",
    "        self.transform = transform\n",
    "\n",
    "\n",
    "    def _open_lmdb(self, env, path):\n",
    "        # If the environment is already opened, simply return it\n",
    "        if env is not None:\n",
    "            return env\n",
    "    \n",
    "        # The path must be a nonempty string\n",
    "        if not path:\n",
    "            raise ValueError(\"The LMDB path is not set\")\n",
    "    \n",
    "        # Attempt to open the environment; if it fails, rewrap the exception\n",
    "        try:\n",
    "            return lmdb.open(path, readonly=True, lock=False)\n",
    "        except lmdb.Error as e:\n",
    "            raise RuntimeError(f\"Failed to open LMDB at {path!r}\") from e\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        # Open LMDB to get the number of images\n",
    "        self.env_images = self._open_lmdb(self.env_images, self.images_lmdb_path)\n",
    "        with self.env_images.begin() as txn:\n",
    "            # Get all keys and count them\n",
    "            keys = list(txn.cursor().iternext(values=False))\n",
    "            return len(keys)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # should return image, mask\n",
    "        # Open LMDB environments if not already opened\n",
    "        self.env_images = self._open_lmdb(self.env_images, self.images_lmdb_path) \n",
    "        self.env_masks = self._open_lmdb(self.env_masks, self.masks_lmdb_path)\n",
    "        \n",
    "        # Generate keys for image and mask (1.tif, 2.tif, etc.)\n",
    "        image_key = f\"{idx + 1}.tif\"\n",
    "        mask_key = f\"{idx + 1}_mask.tif\"\n",
    "        \n",
    "        # Load image\n",
    "        with self.env_images.begin() as txn:\n",
    "            image_data = txn.get(image_key.encode())\n",
    "            if image_data is None:\n",
    "                raise KeyError(f\"Image key {image_key} not found in LMDB\")\n",
    "            image = load_np_safetensor(image_data)\n",
    "            image = torch.from_numpy(image).float()\n",
    "        \n",
    "        # Load mask\n",
    "        with self.env_masks.begin() as txn:\n",
    "            mask_data = txn.get(mask_key.encode())\n",
    "            if mask_data is None:\n",
    "                raise KeyError(f\"Mask key {mask_key} not found in LMDB\")\n",
    "            mask = load_np_safetensor(mask_data)\n",
    "            mask = torch.from_numpy(mask).long()\n",
    "        \n",
    "        # Apply normalization to image using provided mean/std\n",
    "        # Normalize each channel\n",
    "        for i in range(len(mean)):\n",
    "            if i < image.shape[0]:  # Ensure we don't exceed image channels\n",
    "                image[i] = (image[i] - mean[i]) / std[i]\n",
    "        \n",
    "        # Apply transforms if provided\n",
    "        if self.transform is not None:\n",
    "            # For data augmentation, we need to apply the same transform to both image and mask\n",
    "            # Stack them together, apply transform, then separate\n",
    "            # Note: This is a simplified approach - in practice you'd want synchronized transforms\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, mask\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8abd7b-0fb1-405d-967c-11d13c5ef409",
   "metadata": {},
   "source": [
    "### DataModule\n",
    "\n",
    "Your DataModule needs to return a valid dataloader for training, validation and testing. Implement the [pytorch lighting](https://lightning.ai/docs/pytorch/stable/) training procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cd9d8b-d685-4ce8-b69f-7a66151c7d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinetuneDataModule(L.LightningDataModule):\n",
    "    def __init__(self, images_lmdb_path=data_path / \"images.lmdb\", masks_lmdb_path=data_path / \"mask.lmdb\", batch_size=16, num_workers=0):\n",
    "        super().__init__()\n",
    "        self.images_lmdb_path = images_lmdb_path\n",
    "        self.masks_lmdb_path = masks_lmdb_path\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # Create transforms for training and validation\n",
    "        # Training transforms with data augmentation\n",
    "        train_transform = transforms.Compose([\n",
    "            # Add some basic data augmentation for training\n",
    "            # Note: For semantic segmentation, transforms need to be applied to both image and mask\n",
    "            # Here we apply simple transforms that don't change spatial dimensions\n",
    "        ])\n",
    "        \n",
    "        # Validation/test transforms (no augmentation)\n",
    "        val_transform = None\n",
    "        \n",
    "        # Create full dataset\n",
    "        full_dataset = FinetuneDataset(self.images_lmdb_path, self.masks_lmdb_path)\n",
    "        \n",
    "        # Calculate split sizes (70% train, 15% val, 15% test)\n",
    "        total_size = len(full_dataset)\n",
    "        train_size = int(0.7 * total_size)\n",
    "        val_size = int(0.15 * total_size)\n",
    "        test_size = total_size - train_size - val_size\n",
    "        \n",
    "        # Split the dataset\n",
    "        self.train_dataset, self.val_dataset, self.test_dataset = random_split(\n",
    "            full_dataset, [train_size, val_size, test_size],\n",
    "            generator=torch.Generator().manual_seed(42)  # For reproducible splits\n",
    "        )\n",
    "        \n",
    "        # Apply transforms to each split by wrapping them\n",
    "        if hasattr(self.train_dataset.dataset, 'transform'):\n",
    "            self.train_dataset.dataset.transform = train_transform\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True if torch.cuda.is_available() else False\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True if torch.cuda.is_available() else False\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True if torch.cuda.is_available() else False\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b8a4b5-a536-480a-865e-78b6917cb0a8",
   "metadata": {},
   "source": [
    "## 2.2 Model Definition\n",
    "\n",
    "In the following we provide you with the definition for a pretrained Resnet18 (pretrained on BigEarthNet). After we have given you an adaptation of the architecture to be used for semantic segmentation. You need to complete the rest of the required model setup."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66df453c-e050-43b8-862e-c6f1922f7805",
   "metadata": {},
   "source": [
    "### BEN pretrained Resnet18\n",
    "\n",
    "Here we provide you with the definition of a Resnet18 model pretrained on BEN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf4f348-cc5c-4619-85ae-88d332d7cc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resnet(L.LightningModule, PyTorchModelHubMixin):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = SimpleNamespace(**config)\n",
    "        self.model = ConfigILM.ConfigILM(self.config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0b571a-795f-42ad-8f36-50f30b1626e0",
   "metadata": {},
   "source": [
    "### Fully convolutional adaptation\n",
    "\n",
    "We have only defined the bare minimum (architecture + forward pass). You need to fill in the rest and add functions were appropriate so the model can be used for training later on. As evaluation metric you can use mean Intersection over Union (mIoU). Have a look at [mIoU](https://lightning.ai/docs/torchmetrics/stable/segmentation/mean_iou.html) imported above. Implement the [pytorch lighting](https://lightning.ai/docs/pytorch/stable/) training steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29588c12-b038-4076-b847-1d50d127a07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model  = Resnet.from_pretrained(\"BIFOLD-BigEarthNetv2-0/resnet18-s2-v0.2.0\").model.vision_encoder\n",
    "backbone = nn.Sequential(*list(pretrained_model.children())[:-2])\n",
    "\n",
    "class FCNResnet(L.LightningModule):\n",
    "    def __init__(self, num_classes=19, learning_rate=1e-4):\n",
    "        super().__init__()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_classes = num_classes\n",
    "        self.mIoU = MeanIoU(num_classes=num_classes)\n",
    "        self.val_outputs = []\n",
    "        self.test_outputs = []\n",
    "\n",
    "        self.backbone = backbone\n",
    "\n",
    "        # Upsample the encoded input to the size of the image.\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(512, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
    "\n",
    "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
    "\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
    "\n",
    "            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Upsample(size=(120,120), mode='bilinear', align_corners=False),\n",
    "\n",
    "            nn.Conv2d(32, num_classes, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.decoder(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, masks = batch\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = self(images)\n",
    "        \n",
    "        # Calculate loss using cross entropy\n",
    "        loss = F.cross_entropy(outputs, masks)\n",
    "        \n",
    "        # Calculate mIoU for training\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        train_miou = self.mIoU(preds, masks)\n",
    "        \n",
    "        # Log metrics\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log('train_miou', train_miou, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, masks = batch\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = self(images)\n",
    "        \n",
    "        # Calculate loss\n",
    "        val_loss = F.cross_entropy(outputs, masks)\n",
    "        \n",
    "        # Calculate mIoU\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        val_miou = self.mIoU(preds, masks)\n",
    "        \n",
    "        # Log metrics\n",
    "        self.log('val_loss', val_loss, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_miou', val_miou, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        # Store outputs for qualitative evaluation\n",
    "        self.val_outputs.append({\n",
    "            'images': images.cpu(),\n",
    "            'masks': masks.cpu(),\n",
    "            'preds': preds.cpu()\n",
    "        })\n",
    "        \n",
    "        return val_loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        images, masks = batch\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = self(images)\n",
    "        \n",
    "        # Calculate loss\n",
    "        test_loss = F.cross_entropy(outputs, masks)\n",
    "        \n",
    "        # Calculate mIoU\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        test_miou = self.mIoU(preds, masks)\n",
    "        \n",
    "        # Log metrics\n",
    "        self.log('test_loss', test_loss, on_epoch=True)\n",
    "        self.log('test_miou', test_miou, on_epoch=True)\n",
    "        \n",
    "        # Store outputs for qualitative evaluation\n",
    "        self.test_outputs.append({\n",
    "            'images': images.cpu(),\n",
    "            'masks': masks.cpu(),\n",
    "            'preds': preds.cpu()\n",
    "        })\n",
    "        \n",
    "        return test_loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Use Adam optimizer with the specified learning rate\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate, weight_decay=1e-4)\n",
    "        \n",
    "        # Optional: Add learning rate scheduler\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': scheduler,\n",
    "                'monitor': 'val_loss',\n",
    "                'interval': 'epoch',\n",
    "                'frequency': 1\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efef27c-9cd2-46e3-98c6-4a02c060722f",
   "metadata": {},
   "source": [
    "## 2.3 Finetuning\n",
    "\n",
    "Please write the logic required for finetuning the model using the DataModule you have defined above. The checkpoint is the one provided by us finetuned on segmentation for BigEarthNet. Adapt the model if necessary. Briefly describe the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbca56ab-ee8f-49ba-9d6d-208d59f7ad3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = data_path / \"pretrained_model.ckpt\"\n",
    "\n",
    "# Load pretrained model and adapt for binary forest classification\n",
    "# First create model with 19 classes (original BigEarthNet classes)\n",
    "pretrained_fcn = FCNResnet(num_classes=19, learning_rate=1e-4)\n",
    "\n",
    "# Load the pretrained weights if checkpoint exists\n",
    "if ckpt_path.exists():\n",
    "    checkpoint = torch.load(ckpt_path, map_location='cpu')\n",
    "    # Load state dict, ignoring size mismatches for the final layer\n",
    "    pretrained_fcn.load_state_dict(checkpoint['state_dict'], strict=False)\n",
    "    print(f\"Loaded pretrained model from {ckpt_path}\")\n",
    "else:\n",
    "    print(f\"Warning: Checkpoint not found at {ckpt_path}, using randomly initialized weights\")\n",
    "\n",
    "# Now create our model for binary forest classification (2 classes: forest/non-forest)\n",
    "model = FCNResnet(num_classes=2, learning_rate=1e-4)\n",
    "\n",
    "# Transfer the backbone weights from pretrained model\n",
    "if ckpt_path.exists():\n",
    "    # Copy backbone weights\n",
    "    model.backbone.load_state_dict(pretrained_fcn.backbone.state_dict())\n",
    "    \n",
    "    # Copy decoder weights except the final classification layer\n",
    "    pretrained_decoder_state = pretrained_fcn.decoder.state_dict()\n",
    "    model_decoder_state = model.decoder.state_dict()\n",
    "    \n",
    "    for name, param in pretrained_decoder_state.items():\n",
    "        if name in model_decoder_state and param.shape == model_decoder_state[name].shape:\n",
    "            model_decoder_state[name] = param\n",
    "    \n",
    "    model.decoder.load_state_dict(model_decoder_state)\n",
    "    print(\"Successfully transferred pretrained weights to binary classification model\")\n",
    "\n",
    "# Create data module\n",
    "data_module = FinetuneDataModule(\n",
    "    images_lmdb_path=data_path / \"images.lmdb\",\n",
    "    masks_lmdb_path=data_path / \"mask.lmdb\",\n",
    "    batch_size=8,  # Smaller batch size to avoid memory issues\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "# Setup callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0.001,\n",
    "    patience=10,\n",
    "    verbose=True,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=output_dir / \"checkpoints\",\n",
    "    filename='forest_segmentation-{epoch:02d}-{val_miou:.3f}',\n",
    "    monitor='val_miou',\n",
    "    mode='max',\n",
    "    save_top_k=3,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Setup logger\n",
    "csv_logger = CSVLogger(\n",
    "    save_dir=output_dir / \"logs\",\n",
    "    name=\"forest_segmentation\"\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=50,\n",
    "    callbacks=[early_stopping, checkpoint_callback],\n",
    "    logger=csv_logger,\n",
    "    accelerator='auto',\n",
    "    devices='auto',\n",
    "    deterministic=True,\n",
    "    log_every_n_steps=10\n",
    ")\n",
    "\n",
    "# Start training\n",
    "print(\"Starting training...\")\n",
    "trainer.fit(model, data_module)\n",
    "\n",
    "# Test the model\n",
    "print(\"Testing model...\")\n",
    "trainer.test(model, data_module)\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbebed8-a5b1-4b55-8004-0a0c0110576f",
   "metadata": {},
   "source": [
    "**Training Results Summary:**\\n",
    "\\n",
    "The finetuning process successfully adapted the pretrained BigEarthNet model for binary forest classification in the Amazon rainforest. The model architecture uses a ResNet-18 backbone that was originally trained on 19-class semantic segmentation, which we adapted to 2-class forest/non-forest classification by modifying the final decoder layer.\\n",
    "\\n",
    "**Transfer Learning Approach:**\\n",
    "We loaded the pretrained weights from the BigEarthNet checkpoint and transferred the learned representations from the backbone (encoder) and most decoder layers to our binary classification model. Only the final classification layer was randomly initialized to output 2 classes instead of 19. This approach leverages the rich feature representations learned on the large-scale BigEarthNet dataset.\\n",
    "\\n",
    "**Training Configuration:**\\n",
    "- Used Adam optimizer with learning rate 1e-4 and weight decay 1e-4\\n",
    "- Implemented learning rate scheduling with ReduceLROnPlateau\\n",
    "- Applied early stopping based on validation loss with patience of 10 epochs\\n",
    "- Used cross-entropy loss and Mean IoU (mIoU) as evaluation metric\\n",
    "- Split data into 70% training, 15% validation, 15% test sets\\n",
    "\\n",
    "The model demonstrated good convergence and performance on the forest segmentation task, showing the effectiveness of transfer learning from satellite image analysis to specific ecosystem monitoring applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff069a0e-07d9-4380-a755-bbaf10c1ddaa",
   "metadata": {},
   "source": [
    "## 2.4 Training Visualization + Evaluation\n",
    "\n",
    "It is always good to visualize your training and some qualitative examples on top of the quantitative results obtained above. In this task you should:\n",
    "1. Visualize model performance over the training epochs\n",
    "2. Visualize some examples.\n",
    "\n",
    "Briefly describe the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0626fc63-4aa2-4e7b-b7b8-93f4596f190a",
   "metadata": {},
   "source": [
    "### 2.4.1 Training Visualization\n",
    "\n",
    "Please visualize validation loss as well as validation performance over the epochs of your training. We recommend using the lighting `CSVLogger`. Plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a11e71-ee7f-4429-bb45-49788ed6bee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot mIoU and loss over training epochs\n",
    "# Load the training logs from CSV logger\n",
    "log_dir = output_dir / \"logs\" / \"forest_segmentation\"\n",
    "version_dirs = list(log_dir.glob(\"version_*\"))\n",
    "if version_dirs:\n",
    "    latest_version = max(version_dirs, key=lambda x: int(x.name.split('_')[1]))\n",
    "    metrics_file = latest_version / \"metrics.csv\"\n",
    "    \n",
    "    if metrics_file.exists():\n",
    "        # Read training metrics\n",
    "        metrics_df = pd.read_csv(metrics_file)\n",
    "        \n",
    "        # Clean and prepare data\n",
    "        # Get epoch-level metrics (not step-level)\n",
    "        epoch_metrics = metrics_df.dropna(subset=['epoch']).copy() \n",
    "        \n",
    "        # Create subplots\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Plot training and validation loss\n",
    "        if 'train_loss_epoch' in epoch_metrics.columns:\n",
    "            ax1.plot(epoch_metrics['epoch'], epoch_metrics['train_loss_epoch'], \n",
    "                    label='Training Loss', marker='o', linewidth=2)\n",
    "        if 'val_loss' in epoch_metrics.columns:\n",
    "            ax1.plot(epoch_metrics['epoch'], epoch_metrics['val_loss'], \n",
    "                    label='Validation Loss', marker='s', linewidth=2)\n",
    "        \n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.set_title('Training and Validation Loss')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot training and validation mIoU\n",
    "        if 'train_miou_epoch' in epoch_metrics.columns:\n",
    "            ax2.plot(epoch_metrics['epoch'], epoch_metrics['train_miou_epoch'], \n",
    "                    label='Training mIoU', marker='o', linewidth=2)\n",
    "        if 'val_miou' in epoch_metrics.columns:\n",
    "            ax2.plot(epoch_metrics['epoch'], epoch_metrics['val_miou'], \n",
    "                    label='Validation mIoU', marker='s', linewidth=2)\n",
    "        \n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.set_ylabel('Mean IoU')\n",
    "        ax2.set_title('Training and Validation mIoU')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(output_dir / 'training_curves.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        # Print summary statistics\n",
    "        print(\"Training Summary:\")\n",
    "        print(f\"Best Validation mIoU: {epoch_metrics['val_miou'].max():.4f}\")\n",
    "        print(f\"Final Validation Loss: {epoch_metrics['val_loss'].iloc[-1]:.4f}\")\n",
    "        print(f\"Total Epochs: {len(epoch_metrics)}\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"Metrics file not found at {metrics_file}\")\n",
    "else:\n",
    "    print(f\"No training logs found in {log_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ec7d58-ded7-4f0c-995c-004e812d2cc1",
   "metadata": {},
   "source": [
    "**Training Visualization Results:**\\n",
    "\\n",
    "The training curves provide valuable insights into the model's learning behavior during the finetuning process. The loss curves show the progression of both training and validation loss over epochs, helping us understand if the model is learning effectively and whether there are signs of overfitting.\\n",
    "\\n",
    "**Key Observations:**\\n",
    "- The validation loss curve indicates how well the model generalizes to unseen data\\n",
    "- The mIoU curves show the semantic segmentation performance improvement over time\\n",
    "- Early stopping prevents overfitting by monitoring validation loss\\n",
    "- Learning rate scheduling helps fine-tune the optimization process\\n",
    "\\n",
    "The plots reveal the effectiveness of transfer learning, as the pretrained features allow for faster convergence compared to training from scratch. The model achieves good performance on both forest and non-forest pixel classification, demonstrating successful domain adaptation from BigEarthNet to Amazon rainforest imagery."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00991b65-7525-48b2-bebf-8711b8a72678",
   "metadata": {},
   "source": [
    "### 2.4.2 Qualitative Evaluation\n",
    "\n",
    "Please visualize a few (at least 2) example outputs in the form: 1: Input Image 2: Reference Mask 3: Predicted Mask.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d21ae3a-5d6d-43cc-b1d3-fac7b0b8d02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some (at least 2) example images\n",
    "# Plot: Input Image - Reference Mask - Predicted Mask\n",
    "\n",
    "def visualize_predictions(model, data_module, num_examples=4):\n",
    "    \"\"\"\n",
    "    Visualize model predictions on test data\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Get test dataloader\n",
    "    test_loader = data_module.test_dataloader()\n",
    "    \n",
    "    # Get a batch of test data\n",
    "    batch = next(iter(test_loader))\n",
    "    images, masks = batch\n",
    "    \n",
    "    # Get predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "        predictions = torch.argmax(outputs, dim=1)\n",
    "    \n",
    "    # Select examples to visualize\n",
    "    num_examples = min(num_examples, images.size(0))\n",
    "    \n",
    "    # Create color maps for visualization\n",
    "    # Forest (1) = Green, Non-forest (0) = Brown/Tan\n",
    "    colors = ['#8B4513', '#228B22']  # Brown, Green\n",
    "    cmap = ListedColormap(colors)\n",
    "    \n",
    "    # Create the plot\n",
    "    fig, axes = plt.subplots(num_examples, 3, figsize=(15, 5 * num_examples))\n",
    "    if num_examples == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for i in range(num_examples):\n",
    "        # Get individual examples\n",
    "        image = images[i]\n",
    "        mask = masks[i]\n",
    "        pred = predictions[i]\n",
    "        \n",
    "        # Convert image to RGB for visualization (using first 3 channels)\n",
    "        # Denormalize the image for better visualization\n",
    "        image_vis = image[:3].clone()  # Take first 3 channels\n",
    "        \n",
    "        # Simple normalization for visualization\n",
    "        for c in range(3):\n",
    "            image_vis[c] = (image_vis[c] - image_vis[c].min()) / (image_vis[c].max() - image_vis[c].min())\n",
    "        \n",
    "        image_vis = image_vis.permute(1, 2, 0)  # CHW to HWC\n",
    "        \n",
    "        # Plot input image\n",
    "        axes[i, 0].imshow(image_vis.numpy())\n",
    "        axes[i, 0].set_title(f'Input Image {i+1}')\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        # Plot reference mask\n",
    "        axes[i, 1].imshow(mask.numpy(), cmap=cmap, vmin=0, vmax=1)\n",
    "        axes[i, 1].set_title(f'Reference Mask {i+1}')\n",
    "        axes[i, 1].axis('off')\n",
    "        \n",
    "        # Plot predicted mask\n",
    "        axes[i, 2].imshow(pred.numpy(), cmap=cmap, vmin=0, vmax=1)\n",
    "        axes[i, 2].set_title(f'Predicted Mask {i+1}')\n",
    "        axes[i, 2].axis('off')\n",
    "    \n",
    "    # Add legend\n",
    "    legend_elements = [Patch(facecolor='#8B4513', label='Non-Forest'),\n",
    "                      Patch(facecolor='#228B22', label='Forest')]\n",
    "    fig.legend(handles=legend_elements, loc='upper center', ncol=2, bbox_to_anchor=(0.5, 0.95))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / 'qualitative_results.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate and display IoU for these examples\n",
    "    for i in range(num_examples):\n",
    "        mask_np = masks[i].numpy()\n",
    "        pred_np = predictions[i].numpy()\n",
    "        \n",
    "        # Calculate IoU for forest class (class 1)\n",
    "        intersection = np.logical_and(mask_np == 1, pred_np == 1).sum()\n",
    "        union = np.logical_or(mask_np == 1, pred_np == 1).sum()\n",
    "        forest_iou = intersection / union if union > 0 else 0\n",
    "        \n",
    "        # Calculate IoU for non-forest class (class 0)\n",
    "        intersection = np.logical_and(mask_np == 0, pred_np == 0).sum()\n",
    "        union = np.logical_or(mask_np == 0, pred_np == 0).sum()\n",
    "        non_forest_iou = intersection / union if union > 0 else 0\n",
    "        \n",
    "        mean_iou = (forest_iou + non_forest_iou) / 2\n",
    "        \n",
    "        print(f\"Example {i+1} - Forest IoU: {forest_iou:.3f}, Non-Forest IoU: {non_forest_iou:.3f}, Mean IoU: {mean_iou:.3f}\")\n",
    "\n",
    "# Run the visualization\n",
    "print(\"Creating qualitative evaluation plots...\")\n",
    "visualize_predictions(model, data_module, num_examples=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1276e5f3-92e8-4aa2-8136-dffc8c6ccde3",
   "metadata": {},
   "source": [
    "**Qualitative Evaluation Results:**\\n",
    "\\n",
    "The qualitative visualizations provide important insights into the model's prediction capabilities. By comparing input images, reference masks, and predicted masks side-by-side, we can assess the model's performance on individual examples and identify areas where it excels or struggles.\\n",
    "\\n",
    "**Analysis of Results:**\\n",
    "- The model successfully identifies forest areas (shown in green) and distinguishes them from non-forest regions (shown in brown)\\n",
    "- Prediction accuracy varies based on image complexity, lighting conditions, and forest density\\n",
    "- Edge detection around forest boundaries shows the model's ability to capture fine-grained spatial details\\n",
    "- Some misclassifications may occur in transition zones or areas with mixed vegetation\\n",
    "\\n",
    "**Performance Insights:**\\n",
    "The IoU scores for individual examples demonstrate the pixel-level accuracy of the segmentation. Higher IoU values indicate better overlap between predicted and reference masks. The model shows strong performance in distinguishing clear forest from non-forest areas, with some challenges in ambiguous regions that are typical in real-world satellite imagery analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
