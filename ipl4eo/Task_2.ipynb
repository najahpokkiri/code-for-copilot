{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74f2ef0d-8fc7-40b5-9aa4-5babd093f365",
   "metadata": {},
   "source": [
    "# Task 2: CNN Training for pixel-wise classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd45dd54-dcd0-4446-8eae-468280ee243d",
   "metadata": {},
   "source": [
    "In this task you will be provided a model that was pretrained on [BigEarthNet v2](https://arxiv.org/abs/1902.06148) for pixel-wise classification (i.e. semantic segmentation). We will provide you with a checkpoint, as well as the model definition and your task is to load that model using these weights and finetune it on our target domain (forest segmentation) in our target location (Amazon Rainforest) with pytorch lightning. For that we will provide you with a finetuning dataset.\n",
    "\n",
    "<img src=\"../../../data/Example_finetune.png\" alt=\"Example from Dataset\" width=\"600\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e960f3-6a04-4668-9419-868a1577b76e",
   "metadata": {},
   "source": [
    "The goals of this task are as follows:\n",
    "1. Load a pretrained pixelwise segmentation model\n",
    "2. Adapt and finetune the model on a new domain (forest segmentation) and location (Amazon Rain Forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35b29d9-76b8-419a-88e8-2852d169e98a",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "These are all imports we used when solving the task. Please leave them as is even though you might not need all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29eb635d-ec58-4b38-8ca8-72cac53fddd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import rootutils\n",
    "root = rootutils.setup_root(os.path.abspath(''), dotenv=True, pythonpath=True, cwd=False)\n",
    "\n",
    "data_path = root / \"data\"\n",
    "data_path.mkdir(exist_ok=True)\n",
    "output_dir = root / \"output\"\n",
    "output_dir.mkdir(exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69b81d6-6583-427d-bfbd-6b1b09b27aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from types import SimpleNamespace\n",
    "from huggingface_hub import PyTorchModelHubMixin\n",
    "import lightning as L\n",
    "from configilm import ConfigILM # see https://lhackel-tub.github.io/ConfigILM/ for more information\n",
    "from torchinfo import summary\n",
    "\n",
    "from torchmetrics.segmentation import MeanIoU\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import lmdb\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from safetensors.numpy import load as load_np_safetensor\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib.patches import Patch\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376d4556-dfd4-41a1-a1b7-571c08fee0dd",
   "metadata": {},
   "source": [
    "## 2.1 Dataset + DataModule definition\n",
    "\n",
    "Before we can use our data we need to wrap it in a pytorch dataset and thereafter in a lightning DataModule so we can use it for model training. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150d8546-062e-4e46-b89e-33c3f4a036f3",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "For efficient data loading we have put the images in the file `images.lmdb` and the segmentation masks (forest/ no forest) in the file `mask.lmdb`. [LMDB](http://www.lmdb.tech/doc/) is a key-value in-memory database. For the images the key is the image name (1.tif, 2.tif,...) and the values are the image pixels as safetensor (Tip: use `load_np_safetensor` to read it). For the masks the key is the image name followed by _mask (1_mask.tif, 2_mask.tif, ...) the value again is the pixels as safetensor (1 for forest, 0 for no forest). We provided the helper function `_open_lmdb` which opens a connection to the lmdb for images or masks if it does not exist yet. You can read data from the lmdb through `with self.env_images.begin() as txn: txn.get()`. Feel free to add additional functions and adapt the already existing ones. Please open the lmdb only in the `__getitem__` method, due to multi processing.\n",
    "Use preprocessing and data augmentation where applicable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabf9ceb-3a0d-4b23-90f1-e2acd3092a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean =  [438.37207031, 614.05566406, 588.40960693, 2193.29199219, 942.84332275, 1769.93164062, 2049.55151367, 1568.22680664, 997.73248291, 2235.55664062]\n",
    "std = [607.02685547, 603.29681396, 684.56884766, 1369.3717041, 738.43267822, 1100.45605469, 1275.80541992, 1070.16125488, 813.52764893, 1356.54406738]\n",
    "\n",
    "\n",
    "class FinetuneDataset(Dataset):\n",
    "    def __init__(self, images_lmdb_path=data_path / \"images.lmdb\", masks_lmdb_path=data_path / \"mask.lmdb\", transform=None):\n",
    "        self.images_lmdb_path = images_lmdb_path\n",
    "        self.masks_lmdb_path = masks_lmdb_path\n",
    "\n",
    "        self.env_images = None\n",
    "        self.env_masks = None\n",
    "        self.transform = transform\n",
    "\n",
    "\n",
    "    def _open_lmdb(self, env, path):\n",
    "        # If the environment is already opened, simply return it\n",
    "        if env is not None:\n",
    "            return env\n",
    "    \n",
    "        # The path must be a nonempty string\n",
    "        if not path:\n",
    "            raise ValueError(\"The LMDB path is not set\")\n",
    "    \n",
    "        # Attempt to open the environment; if it fails, rewrap the exception\n",
    "        try:\n",
    "            return lmdb.open(path, readonly=True, lock=False)\n",
    "        except lmdb.Error as e:\n",
    "            raise RuntimeError(f\"Failed to open LMDB at {path!r}\") from e\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        pass\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # should return image, mask\n",
    "        pass\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8abd7b-0fb1-405d-967c-11d13c5ef409",
   "metadata": {},
   "source": [
    "### DataModule\n",
    "\n",
    "Your DataModule needs to return a valid dataloader for training, validation and testing. Implement the [pytorch lighting](https://lightning.ai/docs/pytorch/stable/) training procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cd9d8b-d685-4ce8-b69f-7a66151c7d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinetuneDataModule(L.LightningDataModule):\n",
    "    def __init__(self, images_lmdb_path=data_path / \"images.lmdb\", masks_lmdb_path=data_path / \"mask.lmdb\", batch_size=16, num_workers=0):\n",
    "        super().__init__()\n",
    "        self.images_lmdb_path = images_lmdb_path\n",
    "        self.masks_lmdb_path = masks_lmdb_path\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        pass\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        pass\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        pass\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b8a4b5-a536-480a-865e-78b6917cb0a8",
   "metadata": {},
   "source": [
    "## 2.2 Model Definition\n",
    "\n",
    "In the following we provide you with the definition for a pretrained Resnet18 (pretrained on BigEarthNet). After we have given you an adaptation of the architecture to be used for semantic segmentation. You need to complete the rest of the required model setup."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66df453c-e050-43b8-862e-c6f1922f7805",
   "metadata": {},
   "source": [
    "### BEN pretrained Resnet18\n",
    "\n",
    "Here we provide you with the definition of a Resnet18 model pretrained on BEN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf4f348-cc5c-4619-85ae-88d332d7cc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resnet(L.LightningModule, PyTorchModelHubMixin):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = SimpleNamespace(**config)\n",
    "        self.model = ConfigILM.ConfigILM(self.config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0b571a-795f-42ad-8f36-50f30b1626e0",
   "metadata": {},
   "source": [
    "### Fully convolutional adaptation\n",
    "\n",
    "We have only defined the bare minimum (architecture + forward pass). You need to fill in the rest and add functions were appropriate so the model can be used for training later on. As evaluation metric you can use mean Intersection over Union (mIoU). Have a look at [mIoU](https://lightning.ai/docs/torchmetrics/stable/segmentation/mean_iou.html) imported above. Implement the [pytorch lighting](https://lightning.ai/docs/pytorch/stable/) training steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29588c12-b038-4076-b847-1d50d127a07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model  = Resnet.from_pretrained(\"BIFOLD-BigEarthNetv2-0/resnet18-s2-v0.2.0\").model.vision_encoder\n",
    "backbone = nn.Sequential(*list(pretrained_model.children())[:-2])\n",
    "\n",
    "class FCNResnet(L.LightningModule):\n",
    "    def __init__(self, num_classes=19, learning_rate=1e-4):\n",
    "        super().__init__()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_classes = num_classes\n",
    "        self.mIoU = MeanIoU(num_classes=num_classes)\n",
    "        self.val_outputs = []\n",
    "        self.test_outputs = []\n",
    "\n",
    "        self.backbone = backbone\n",
    "\n",
    "        # Upsample the encoded input to the size of the image.\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(512, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
    "\n",
    "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
    "\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
    "\n",
    "            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Upsample(size=(120,120), mode='bilinear', align_corners=False),\n",
    "\n",
    "            nn.Conv2d(32, num_classes, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.decoder(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        pass\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        pass\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        pass\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efef27c-9cd2-46e3-98c6-4a02c060722f",
   "metadata": {},
   "source": [
    "## 2.3 Finetuning\n",
    "\n",
    "Please write the logic required for finetuning the model using the DataModule you have defined above. The checkpoint is the one provided by us finetuned on segmentation for BigEarthNet. Adapt the model if necessary. Briefly describe the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbca56ab-ee8f-49ba-9d6d-208d59f7ad3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = data_path / \"pretrained_model.ckpt\"\n",
    "model = None\n",
    "\n",
    "trainer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbebed8-a5b1-4b55-8004-0a0c0110576f",
   "metadata": {},
   "source": [
    "**TODO:** Describe the results in a few paragraphs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff069a0e-07d9-4380-a755-bbaf10c1ddaa",
   "metadata": {},
   "source": [
    "## 2.4 Training Visualization + Evaluation\n",
    "\n",
    "It is always good to visualize your training and some qualitative examples on top of the quantitative results obtained above. In this task you should:\n",
    "1. Visualize model performance over the training epochs\n",
    "2. Visualize some examples.\n",
    "\n",
    "Briefly describe the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0626fc63-4aa2-4e7b-b7b8-93f4596f190a",
   "metadata": {},
   "source": [
    "### 2.4.1 Training Visualization\n",
    "\n",
    "Please visualize validation loss as well as validation performance over the epochs of your training. We recommend using the lighting `CSVLogger`. Plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a11e71-ee7f-4429-bb45-49788ed6bee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot mIoU and loss over training epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ec7d58-ded7-4f0c-995c-004e812d2cc1",
   "metadata": {},
   "source": [
    "**TODO:** Describe the results in a few paragraphs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00991b65-7525-48b2-bebf-8711b8a72678",
   "metadata": {},
   "source": [
    "### 2.4.2 Qualitative Evaluation\n",
    "\n",
    "Please visualize a few (at least 2) example outputs in the form: 1: Input Image 2: Reference Mask 3: Predicted Mask.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d21ae3a-5d6d-43cc-b1d3-fac7b0b8d02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some (at least 2) example images\n",
    "# Plot: Input Image - Reference Mask - Predicted Mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1276e5f3-92e8-4aa2-8136-dffc8c6ccde3",
   "metadata": {},
   "source": [
    "**TODO:** Describe the results in a few paragraphs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
