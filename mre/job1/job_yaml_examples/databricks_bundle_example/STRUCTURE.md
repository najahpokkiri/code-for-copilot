# Complete Databricks Bundle Structure

This is the **complete, ready-to-deploy** Databricks Asset Bundle including all scripts.

## ğŸ“ Complete Directory Structure

```
databricks_bundle_example/
â”œâ”€â”€ databricks.yml                    # Root bundle configuration
â”‚                                     # - Defines environments (dev/staging/prod)
â”‚                                     # - Sets variables (catalog, schema, country)
â”‚                                     # - Configures workspace sync
â”‚
â”œâ”€â”€ config.yaml                       # Pipeline configuration (source of truth)
â”‚                                     # - Input file paths
â”‚                                     # - Processing parameters
â”‚                                     # - Used by config_builder.py to generate config.json
â”‚
â”œâ”€â”€ resources/
â”‚   â”œâ”€â”€ jobs/
â”‚   â”‚   â””â”€â”€ building_enrichment.yml   # Job definition
â”‚   â”‚                                 # - All 8 tasks (0-7)
â”‚   â”‚                                 # - Task dependencies
â”‚   â”‚                                 # - Library requirements
â”‚   â”‚
â”‚   â””â”€â”€ clusters/
â”‚       â””â”€â”€ main_cluster.yml          # Cluster configuration
â”‚                                     # - Node types, worker counts
â”‚                                     # - Spark configs
â”‚
â”œâ”€â”€ src/                              # Python scripts (pipeline tasks)
â”‚   â”œâ”€â”€ config_builder.py             # Task 0: Generates config.json from config.yaml
â”‚   â”œâ”€â”€ task1_proportions_to_delta.py # Task 1: Load proportions to Delta
â”‚   â”œâ”€â”€ task2_grid_generation.py      # Task 2: Generate grid centroids
â”‚   â”œâ”€â”€ task3_tile_downloader.py      # Task 3: Download GHSL tiles
â”‚   â”œâ”€â”€ task4_raster_stats.py         # Task 4: Extract raster statistics
â”‚   â”œâ”€â”€ task5_post_processing.py      # Task 5: Calculate estimates
â”‚   â”œâ”€â”€ task6_create_views.py         # Task 6: Create SQL views
â”‚   â””â”€â”€ task7_export.py               # Task 7: Export results
â”‚
â”œâ”€â”€ .gitignore                        # Git exclusions
â”‚                                     # - config.json (generated)
â”‚                                     # - __pycache__, logs
â”‚
â””â”€â”€ README.md                         # Deployment instructions

Generated files (not in git):
â”œâ”€â”€ config.json                       # Generated by Task 0 at runtime
â””â”€â”€ .databricks/                      # Bundle deployment metadata
```

## ğŸ“Š File Count

```
Configuration:   3 files  (databricks.yml, 2x resources/*.yml)
Source Code:     8 files  (config_builder.py, task1-7.py)
Documentation:   2 files  (README.md, .gitignore)
Input Config:    1 file   (config.yaml)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total:          14 files
```

## ğŸš€ How Files Are Used

### 1. Development Time (Local)

```
You edit:
  config.yaml                â†’ Pipeline parameters
  resources/jobs/*.yml       â†’ Task definitions
  src/task*.py               â†’ Task logic (if needed)

You commit to git:
  âœ… All of the above
  âŒ config.json (generated)
```

### 2. Deployment Time (databricks bundle deploy)

```
Databricks CLI:
  1. Reads: databricks.yml
  2. Includes: resources/**/*.yml
  3. Uploads: src/**/*.py to workspace
  4. Creates: Job in Databricks

Result:
  - Job created in workspace
  - Scripts uploaded to workspace_path
  - Ready to run
```

### 3. Runtime (Job Execution)

```
Task 0 (config_generation):
  Reads:  config.yaml (from workspace)
  Runs:   src/config_builder.py
  Writes: config.json (to workspace)

Task 1-7:
  Read:   config.json (generated by Task 0)
  Run:    src/task*.py
  Write:  Delta tables, views, exports
```

## ğŸ”„ Two Deployment Approaches

### Approach A: Bundle Sync (Recommended)

In `databricks.yml`:
```yaml
sync:
  include:
    - "src/**/*.py"
    - "*.yaml"
```

Job YAML references uploaded files:
```yaml
python_file: /Workspace/Users/${workspace.current_user.userName}/.bundle/${bundle.name}/${bundle.target}/files/src/config_builder.py
```

**Advantage**: Everything self-contained in bundle

### Approach B: Workspace Reference (Current Example)

Job YAML references existing workspace files:
```yaml
python_file: ${var.workspace_path}/config_builder.py
```

**Advantage**: Works with existing workspace structure

## ğŸ“ Deployment Methods

### Method 1: Self-Contained Bundle (Upload Everything)

```bash
# Bundle includes all scripts
databricks bundle deploy

# Scripts uploaded to:
# /Workspace/Users/you/.bundle/building_enrichment/dev/files/src/
```

**When to use**: Fresh deployment, isolated environment

### Method 2: Reference Existing Scripts

```bash
# Upload scripts separately to workspace
databricks workspace import-dir src/ /Workspace/Users/you/project/scripts/

# Bundle references them via ${var.workspace_path}
databricks bundle deploy
```

**When to use**: Scripts already in workspace, shared across jobs

## ğŸ¯ Current Example Uses

**Method 2** - References existing workspace paths via variables:

```yaml
# In databricks.yml
variables:
  workspace_path: /Workspace/Users/npokkiri@munichre.com/inventory_nos_db/scripts

# In resources/jobs/building_enrichment.yml
python_file: ${var.workspace_path}/config_builder.py
```

## ğŸ”§ How to Switch to Method 1

If you want everything self-contained in the bundle:

1. **Update databricks.yml**:
   ```yaml
   sync:
     include:
       - "src/**/*.py"
       - "config.yaml"
   ```

2. **Update job YAML paths**:
   ```yaml
   # Change from:
   python_file: ${var.workspace_path}/config_builder.py

   # To:
   python_file: ${workspace.root_path}/files/src/config_builder.py
   ```

3. **Deploy**:
   ```bash
   databricks bundle deploy
   # All scripts auto-uploaded
   ```

## ğŸ“¦ Complete File Manifest

| File | Purpose | In Git? | Generated? |
|------|---------|---------|------------|
| `databricks.yml` | Bundle config | âœ… Yes | âŒ No |
| `config.yaml` | Pipeline config | âœ… Yes | âŒ No |
| `resources/jobs/building_enrichment.yml` | Job def | âœ… Yes | âŒ No |
| `resources/clusters/main_cluster.yml` | Cluster config | âœ… Yes | âŒ No |
| `src/config_builder.py` | Task 0 script | âœ… Yes | âŒ No |
| `src/task1_proportions_to_delta.py` | Task 1 script | âœ… Yes | âŒ No |
| `src/task2_grid_generation.py` | Task 2 script | âœ… Yes | âŒ No |
| `src/task3_tile_downloader.py` | Task 3 script | âœ… Yes | âŒ No |
| `src/task4_raster_stats.py` | Task 4 script | âœ… Yes | âŒ No |
| `src/task5_post_processing.py` | Task 5 script | âœ… Yes | âŒ No |
| `src/task6_create_views.py` | Task 6 script | âœ… Yes | âŒ No |
| `src/task7_export.py` | Task 7 script | âœ… Yes | âŒ No |
| `config.json` | Generated config | âŒ No | âœ… Yes (Task 0) |
| `.databricks/` | Deploy metadata | âŒ No | âœ… Yes (CLI) |
| `.gitignore` | Git exclusions | âœ… Yes | âŒ No |
| `README.md` | Documentation | âœ… Yes | âŒ No |

## ğŸ“ Summary

**Now you have**:
- âœ… Complete bundle structure with all files
- âœ… All 8 Python scripts in `src/`
- âœ… config.yaml for pipeline config
- âœ… Proper .gitignore

**Missing from before**:
- âŒ Scripts weren't in the example
- âŒ Directory tree was conceptual, not actual

**Now fixed**:
- âœ… All scripts copied to `src/`
- âœ… config.yaml included
- âœ… Complete, ready-to-deploy structure

**Next steps**:
1. Review the structure: `ls -R databricks_bundle_example/`
2. Customize `databricks.yml` with your workspace details
3. Test deployment: `databricks bundle deploy`
4. Run pipeline: `databricks bundle run building_enrichment_IND`
