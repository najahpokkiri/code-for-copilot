{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Building Enrichment Pipeline - Simple Job Creator\n\n**Quick Start:**\n1. Replace `data/NOS_storey_mapping.csv` with your country-specific file\n2. Update `ISO3` in Cell 2 to your country code\n3. Run all cells\n4. Monitor job progress\n\n**The job will automatically:**\n- Use files from `data/` folder (tsi.csv, admin boundaries already included)\n- Create {ISO3}/input/, {ISO3}/output/, {ISO3}/logs/ folders\n- Copy files to correct locations\n- Generate full config.json with ISO3 suffixes\n- Run the complete pipeline\n\n**Note:** All required data files (except your NOS file) are already in the `data/` folder!"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-install notebook dependencies\n",
    "try:\n",
    "    import databricks.sdk\n",
    "    import yaml\n",
    "    print(\"‚úÖ All dependencies available\")\n",
    "except ImportError:\n",
    "    print(\"Installing packages...\")\n",
    "    %pip install databricks-sdk pyyaml --quiet\n",
    "    dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Configuration (EDIT THIS!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# USER CONFIGURATION - Edit these values\n# ============================================================================\n\n# Run mode: \"test\" or \"full\"\n# - test: Process only 1 tile with 10k grid cells for quick validation\n# - full: Process all tiles for complete country coverage\nRUN_MODE = \"test\"  # Change to \"full\" for production run\n\n# Country code (CHANGE THIS for your country)\nISO3 = \"IND\"\n\n# Databricks settings\nCATALOG = \"prp_mr_bdap_projects\"\nSCHEMA = \"geospatialsolutions\"\nVOLUME_BASE = \"/Volumes/prp_mr_bdap_projects/geospatialsolutions/external/jrc/data\"\n\n# Workspace path (where these scripts are located)\n# IMPORTANT: \n# - For Databricks: Use workspace path like \"/Workspace/Users/yourname/project/mre/job1\"\n# - For local development: Use absolute path to the mre/job1 directory\n#   Example: \"/home/user/code-for-copilot/mre/job1\"\nWORKSPACE_BASE = \"/Workspace/Users/npokkiri@munichre.com/inventory_nos_db/code-for-copilot-main/mre/job1\"\n\n# ============================================================================\n# Input files from data/ folder\n# Just replace NOS_storey_mapping.csv in the data/ folder with your file!\n# ============================================================================\nPROPORTIONS_CSV = f\"{WORKSPACE_BASE}/data/NOS_storey_mapping.csv\"\nTSI_CSV = f\"{WORKSPACE_BASE}/data/tsi.csv\"\nADMIN_BOUNDARIES = f\"{WORKSPACE_BASE}/data/RMS_Admin0_geozones.json.gz\"\n\n# Optional: Email for notifications\nEMAIL = \"npokkiri@munichre.com\"\n\n# Optional: Cluster ID (leave empty to auto-detect)\nCLUSTER_ID = \"\"  # Will auto-detect current cluster if empty\n\n# ============================================================================\n# Processing parameters (optional - defaults provided)\n# ============================================================================\nCELL_SIZE = 2000              # Grid cell size in meters (2km default)\nDOWNLOAD_CONCURRENCY = 3      # Parallel tile downloads\nMAX_WORKERS = 8               # Raster processing threads\nTILE_PARALLELISM = 4          # Concurrent tile processing\n\n# Test mode overrides (automatically set if RUN_MODE=\"test\")\nif RUN_MODE.lower() == \"test\":\n    SAMPLE_SIZE = 10000        # Limit to 10k grid cells\n    MAX_TILES = 1              # Process only 1 tile\n    print(\"‚ö†Ô∏è  TEST MODE: Will process only 1 tile with 10k grid cells\")\nelse:\n    SAMPLE_SIZE = None         # No limit - process all\n    MAX_TILES = None           # Process all tiles\n    print(\"‚úÖ FULL MODE: Will process all tiles for complete coverage\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Initialize & Auto-Detect Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nimport os\nfrom pyspark.sql import SparkSession\n\n# Add workspace base to path for helper imports\n# Convert Workspace path to actual filesystem path if needed\nif WORKSPACE_BASE.startswith(\"/Workspace/\"):\n    # In Databricks, /Workspace/ paths map to actual filesystem\n    actual_path = WORKSPACE_BASE\nelse:\n    # For local/cloned repos, use the path as-is\n    actual_path = WORKSPACE_BASE\n\n# Also try to add current working directory (where notebook is running)\ncurrent_dir = os.getcwd()\nfor path in [actual_path, current_dir]:\n    if path and path not in sys.path:\n        sys.path.insert(0, path)\n        print(f\"üìÅ Added to Python path: {path}\")\n\n# Initialize Spark\nspark = SparkSession.builder.getOrCreate()\n\n# Auto-detect cluster if not specified\nif not CLUSTER_ID:\n    CLUSTER_ID = spark.conf.get(\"spark.databricks.clusterUsageTags.clusterId\")\n    print(f\"üîç Auto-detected cluster ID: {CLUSTER_ID}\")\nelse:\n    print(f\"üìå Using specified cluster ID: {CLUSTER_ID}\")\n\nprint(f\"‚úÖ Configuration loaded for {ISO3}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 4: Validate Helper Modules"
  },
  {
   "cell_type": "code",
   "source": "# Validate that helper modules can be imported\nprint(\"üîç Validating helper modules...\\n\")\n\ntry:\n    import config_generator\n    print(\"‚úÖ config_generator module found\")\nexcept ImportError as e:\n    print(f\"‚ùå config_generator import failed: {e}\")\n    print(f\"\\nüí° Troubleshooting:\")\n    print(f\"   1. Ensure WORKSPACE_BASE points to the directory containing:\")\n    print(f\"      - config_generator.py\")\n    print(f\"      - job_creator.py\") \n    print(f\"      - job_monitor.py\")\n    print(f\"   2. Current WORKSPACE_BASE: {WORKSPACE_BASE}\")\n    print(f\"   3. Current working directory: {os.getcwd()}\")\n    print(f\"   4. Python sys.path: {sys.path[:3]}...\")\n    raise\n\ntry:\n    import job_creator\n    print(\"‚úÖ job_creator module found\")\nexcept ImportError as e:\n    print(f\"‚ùå job_creator import failed: {e}\")\n    raise\n\ntry:\n    import job_monitor\n    print(\"‚úÖ job_monitor module found\")\nexcept ImportError as e:\n    print(f\"‚ùå job_monitor import failed: {e}\")\n    raise\n\nprint(\"\\n‚úÖ All helper modules validated successfully!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Step 5: Generate Minimal Config"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create Databricks Job"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Step 6: Create Databricks Job"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Run Job & Monitor Progress"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Step 7: Run Job & Monitor Progress"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Verify Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Step 8: Verify Outputs"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "generated_config_path = f\"{VOLUME_BASE}/{ISO3}/config.json\"\n\nprint(\"=\"*60)\nprint(\"PIPELINE EXECUTION SUMMARY\")\nprint(\"=\"*60)\nprint(f\"Country: {ISO3}\")\nprint(f\"Job ID: {JOB_ID}\")\nprint(f\"Run ID: {RUN_ID}\")\nprint(f\"\")\nprint(f\"üìÅ Data Location: {VOLUME_BASE}/{ISO3}\")\nprint(f\"üìä Main Output Table: {output_table}\")\nprint(f\"üìÇ Exports: {VOLUME_BASE}/{ISO3}/outputs/exports/{ISO3}/\")\nprint(f\"‚öôÔ∏è  Config: {generated_config_path}\")\nprint(f\"\")\nprint(f\"View job in Databricks UI: Workflows ‚Üí Jobs ‚Üí Building_Enrichment_{ISO3}\")\nprint(\"=\"*60)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}