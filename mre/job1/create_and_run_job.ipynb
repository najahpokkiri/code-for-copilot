{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Building Enrichment Pipeline - Simple Job Creator\n\n**Quick Start:**\n1. Replace `data/NOS_storey_mapping.csv` with your country-specific file\n2. Update `ISO3` in Cell 2 to your country code\n3. Run all cells\n4. Monitor job progress\n\n**The job will automatically:**\n- Use files from `data/` folder (tsi.csv, admin boundaries already included)\n- Create {ISO3}/input/, {ISO3}/output/, {ISO3}/logs/ folders\n- Copy files to correct locations\n- Generate full config.json with ISO3 suffixes\n- Run the complete pipeline\n\n**Note:** All required data files (except your NOS file) are already in the `data/` folder!"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-install notebook dependencies\n",
    "try:\n",
    "    import databricks.sdk\n",
    "    import yaml\n",
    "    print(\"‚úÖ All dependencies available\")\n",
    "except ImportError:\n",
    "    print(\"Installing packages...\")\n",
    "    %pip install databricks-sdk pyyaml --quiet\n",
    "    dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Configuration (EDIT THIS!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# USER CONFIGURATION - Edit these values\n# ============================================================================\n\n# Run mode: \"test\" or \"full\"\n# - test: Process only 1 tile with 10k grid cells for quick validation\n# - full: Process all tiles for complete country coverage\nRUN_MODE = \"test\"  # Change to \"full\" for production run\n\n# Country code (CHANGE THIS for your country)\nISO3 = \"IND\"\n\n# Databricks settings\nCATALOG = \"prp_mr_bdap_projects\"\nSCHEMA = \"geospatialsolutions\"\nVOLUME_BASE = \"/Volumes/prp_mr_bdap_projects/geospatialsolutions/external/jrc/data\"\n\n# Workspace path (where these scripts are located)\nWORKSPACE_BASE = \"/Workspace/Users/npokkiri@munichre.com/inventory_nos_db/code-for-copilot-main/mre/job1\"\n\n# ============================================================================\n# Input files from data/ folder\n# Just replace NOS_storey_mapping.csv in the data/ folder with your file!\n# ============================================================================\nPROPORTIONS_CSV = f\"{WORKSPACE_BASE}/data/NOS_storey_mapping.csv\"\nTSI_CSV = f\"{WORKSPACE_BASE}/data/tsi.csv\"\nADMIN_BOUNDARIES = f\"{WORKSPACE_BASE}/data/RMS_Admin0_geozones.json.gz\"\n\n# Optional: Email for notifications\nEMAIL = \"npokkiri@munichre.com\"\n\n# Optional: Cluster ID (leave empty to auto-detect)\nCLUSTER_ID = \"\"  # Will auto-detect current cluster if empty\n\n# ============================================================================\n# Processing parameters (optional - defaults provided)\n# ============================================================================\nCELL_SIZE = 2000              # Grid cell size in meters (2km default)\nDOWNLOAD_CONCURRENCY = 3      # Parallel tile downloads\nMAX_WORKERS = 8               # Raster processing threads\nTILE_PARALLELISM = 4          # Concurrent tile processing\n\n# Test mode overrides (automatically set if RUN_MODE=\"test\")\nif RUN_MODE.lower() == \"test\":\n    SAMPLE_SIZE = 10000        # Limit to 10k grid cells\n    MAX_TILES = 1              # Process only 1 tile\n    print(\"‚ö†Ô∏è  TEST MODE: Will process only 1 tile with 10k grid cells\")\nelse:\n    SAMPLE_SIZE = None         # No limit - process all\n    MAX_TILES = None           # Process all tiles\n    print(\"‚úÖ FULL MODE: Will process all tiles for complete coverage\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Initialize & Auto-Detect Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Add workspace base to path for helper imports\n",
    "sys.path.insert(0, WORKSPACE_BASE.replace(\"/Workspace\", \"/Workspace\"))\n",
    "\n",
    "# Initialize Spark\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Auto-detect cluster if not specified\n",
    "if not CLUSTER_ID:\n",
    "    CLUSTER_ID = spark.conf.get(\"spark.databricks.clusterUsageTags.clusterId\")\n",
    "    print(f\"üîç Auto-detected cluster ID: {CLUSTER_ID}\")\n",
    "else:\n",
    "    print(f\"üìå Using specified cluster ID: {CLUSTER_ID}\")\n",
    "\n",
    "print(f\"‚úÖ Configuration loaded for {ISO3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Generate Minimal Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config_generator import generate_minimal_config, save_config_to_workspace\n",
    "\n",
    "print(\"‚öôÔ∏è  Generating minimal configuration...\")\n",
    "\n",
    "# Generate config using helper\n",
    "minimal_config = generate_minimal_config(\n",
    "    iso3=ISO3,\n",
    "    catalog=CATALOG,\n",
    "    schema=SCHEMA,\n",
    "    volume_base=VOLUME_BASE,\n",
    "    workspace_base=WORKSPACE_BASE,\n",
    "    proportions_csv=PROPORTIONS_CSV,\n",
    "    tsi_csv=TSI_CSV,\n",
    "    admin_boundaries=ADMIN_BOUNDARIES,\n",
    "    run_mode=RUN_MODE,\n",
    "    cell_size=CELL_SIZE,\n",
    "    download_concurrency=DOWNLOAD_CONCURRENCY,\n",
    "    max_workers=MAX_WORKERS,\n",
    "    tile_parallelism=TILE_PARALLELISM,\n",
    "    sample_size=SAMPLE_SIZE,\n",
    "    max_tiles=MAX_TILES\n",
    ")\n",
    "\n",
    "# Save config to workspace\n",
    "workspace_config_path = save_config_to_workspace(minimal_config, ISO3, WORKSPACE_BASE, dbutils)\n",
    "\n",
    "print(f\"‚úÖ Minimal config created\")\n",
    "print(f\"üìç Location: {workspace_config_path}\")\n",
    "print(f\"üîß Run mode: {RUN_MODE.upper()}\")\n",
    "if RUN_MODE.lower() == \"test\":\n",
    "    print(f\"   - Sample size: {SAMPLE_SIZE:,} grid cells\")\n",
    "    print(f\"   - Max tiles: {MAX_TILES}\")\n",
    "print(f\"\\nThis will be used by Task 0 to generate full config.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create Databricks Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from job_creator import create_databricks_job\n",
    "\n",
    "print(\"üî® Creating Databricks job...\")\n",
    "\n",
    "# Create job using helper\n",
    "JOB_ID, job_name = create_databricks_job(\n",
    "    iso3=ISO3,\n",
    "    cluster_id=CLUSTER_ID,\n",
    "    workspace_base=WORKSPACE_BASE,\n",
    "    catalog=CATALOG,\n",
    "    schema=SCHEMA,\n",
    "    volume_base=VOLUME_BASE,\n",
    "    minimal_config_path=workspace_config_path,\n",
    "    email=EMAIL\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Job created successfully!\")\n",
    "print(f\"   Job ID: {JOB_ID}\")\n",
    "print(f\"   Job Name: {job_name}\")\n",
    "print(f\"   Tasks: 8 (including Task 0 setup)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Run Job & Monitor Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "from job_monitor import monitor_job_progress\n",
    "\n",
    "w = WorkspaceClient()\n",
    "\n",
    "print(f\"üöÄ Starting job {JOB_ID}...\")\n",
    "\n",
    "# Run the job\n",
    "run = w.jobs.run_now(job_id=JOB_ID)\n",
    "RUN_ID = run.run_id\n",
    "\n",
    "print(f\"‚úÖ Job started!\")\n",
    "print(f\"   Run ID: {RUN_ID}\")\n",
    "print()\n",
    "\n",
    "# Monitor progress using helper\n",
    "success, duration, result_state = monitor_job_progress(RUN_ID, update_interval=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Verify Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç Verifying outputs...\\n\")\n",
    "\n",
    "# Check main output table\n",
    "output_table = f\"{CATALOG}.{SCHEMA}.building_enrichment_output_{ISO3}\"\n",
    "\n",
    "try:\n",
    "    df = spark.table(output_table)\n",
    "    count = df.count()\n",
    "    print(f\"‚úÖ Main output table exists: {output_table}\")\n",
    "    print(f\"   Row count: {count:,}\\n\")\n",
    "    print(f\"   Sample data:\")\n",
    "    display(df.limit(5))\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Could not verify table: {e}\")\n",
    "\n",
    "print(f\"\\nüìä Export files location: {VOLUME_BASE}/{ISO3}/output/exports/FULL_{ISO3}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_config_path = f\"{VOLUME_BASE}/{ISO3}/config.json\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PIPELINE EXECUTION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Country: {ISO3}\")\n",
    "print(f\"Job ID: {JOB_ID}\")\n",
    "print(f\"Run ID: {RUN_ID}\")\n",
    "print(f\"\")\n",
    "print(f\"üìÅ Data Location: {VOLUME_BASE}/{ISO3}\")\n",
    "print(f\"üìä Main Output Table: {output_table}\")\n",
    "print(f\"üìÇ Exports: {VOLUME_BASE}/{ISO3}/output/exports/FULL_{ISO3}/\")\n",
    "print(f\"‚öôÔ∏è  Config: {generated_config_path}\")\n",
    "print(f\"\")\n",
    "print(f\"View job in Databricks UI: Workflows ‚Üí Jobs ‚Üí Building_Enrichment_{ISO3}\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}