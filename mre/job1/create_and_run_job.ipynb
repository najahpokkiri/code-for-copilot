{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Enrichment Pipeline - Simple Job Creator\n",
    "\n",
    "**Instructions:**\n",
    "1. Fill out the configuration in Cell 2 below\n",
    "2. Run all cells\n",
    "3. Monitor job progress\n",
    "\n",
    "**The job will automatically:**\n",
    "- Create {ISO3}/input/, {ISO3}/output/, {ISO3}/logs/ folders\n",
    "- Copy files to correct locations\n",
    "- Generate full config.json with ISO3 suffixes\n",
    "- Run the complete pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-install notebook dependencies\n",
    "try:\n",
    "    import databricks.sdk\n",
    "    import yaml\n",
    "    print(\"‚úÖ All dependencies available\")\n",
    "except ImportError:\n",
    "    print(\"Installing packages...\")\n",
    "    %pip install databricks-sdk pyyaml --quiet\n",
    "    dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Configuration (EDIT THIS!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# USER CONFIGURATION - Edit these values\n",
    "# ============================================================================\n",
    "\n",
    "# Country code\n",
    "ISO3 = \"IND\"\n",
    "\n",
    "# Databricks settings\n",
    "CATALOG = \"prp_mr_bdap_projects\"\n",
    "SCHEMA = \"geospatialsolutions\"\n",
    "VOLUME_BASE = \"/Volumes/prp_mr_bdap_projects/geospatialsolutions/external/jrc/data\"\n",
    "\n",
    "# Input file paths (full paths)\n",
    "PROPORTIONS_CSV = \"/Workspace/Users/npokkiri@munichre.com/inventory_nos_db/data/IND_NOS_storey_mapping_041125.csv\"\n",
    "TSI_CSV = \"/Volumes/prp_mr_bdap_projects/geospatialsolutions/external/jrc/data/inputs/multipliers/tsi.csv\"\n",
    "ADMIN_BOUNDARIES = \"/Volumes/prp_mr_bdap_projects/geospatialsolutions/external/jrc/data/inputs/admin/RMS_Admin0_geozones.gpkg\"\n",
    "\n",
    "# Workspace path (where these scripts are located)\n",
    "WORKSPACE_BASE = \"/Workspace/Users/npokkiri@munichre.com/inventory_nos_db/code-for-copilot-main/mre/job1\"\n",
    "\n",
    "# Optional: Email for notifications\n",
    "EMAIL = \"npokkiri@munichre.com\"\n",
    "\n",
    "# Optional: Cluster ID (leave empty to auto-detect)\n",
    "CLUSTER_ID = \"\"  # Will auto-detect current cluster if empty\n",
    "\n",
    "# ============================================================================\n",
    "# Processing parameters (optional - defaults provided)\n",
    "# ============================================================================\n",
    "CELL_SIZE = 2000              # Grid cell size in meters\n",
    "DOWNLOAD_CONCURRENCY = 3      # Parallel tile downloads\n",
    "MAX_WORKERS = 8               # Raster processing threads\n",
    "TILE_PARALLELISM = 4          # Concurrent tile processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Initialize & Auto-Detect Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import json\nimport time\nimport yaml\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service.jobs import Task, TaskDependency, SparkPythonTask, Library\nfrom pyspark.sql import SparkSession\n\n# Initialize\nspark = SparkSession.builder.getOrCreate()\nw = WorkspaceClient()\n\n# Auto-detect cluster if not specified\nif not CLUSTER_ID:\n    CLUSTER_ID = spark.conf.get(\"spark.databricks.clusterUsageTags.clusterId\")\n    print(f\"üîç Auto-detected cluster ID: {CLUSTER_ID}\")\nelse:\n    print(f\"üìå Using specified cluster ID: {CLUSTER_ID}\")\n\nprint(f\"‚úÖ Configuration loaded for {ISO3}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Generate Minimal Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create minimal config (like config.yaml)\n",
    "minimal_config = {\n",
    "    \"project\": {\n",
    "        \"catalog\": CATALOG,\n",
    "        \"schema\": SCHEMA,\n",
    "        \"volume_root\": VOLUME_BASE\n",
    "    },\n",
    "    \"country\": {\n",
    "        \"iso3\": ISO3\n",
    "    },\n",
    "    \"inputs\": {\n",
    "        \"proportions_csv\": PROPORTIONS_CSV,\n",
    "        \"tsi_csv\": TSI_CSV,\n",
    "        \"admin_boundaries\": ADMIN_BOUNDARIES,\n",
    "        \"tile_footprint\": f\"{WORKSPACE_BASE}/ghsl2_0_mwd_l1_tile_schema_land.gpkg\"\n",
    "    },\n",
    "    \"params\": {\n",
    "        \"cell_size\": CELL_SIZE,\n",
    "        \"export_crs\": \"EPSG:4326\",\n",
    "        \"target_crs\": \"ESRI:54009\",\n",
    "        \"datasets\": \"built_c,smod\",\n",
    "        \"download_concurrency\": DOWNLOAD_CONCURRENCY,\n",
    "        \"download_retries\": 2,\n",
    "        \"use_smod\": True,\n",
    "        \"use_boundary_mask\": True,\n",
    "        \"include_nodata\": True,\n",
    "        \"add_percentages\": False,\n",
    "        \"chunk_size\": 10000,\n",
    "        \"max_workers\": MAX_WORKERS,\n",
    "        \"tile_parallelism\": TILE_PARALLELISM,\n",
    "        \"sample_size\": 10000,\n",
    "        \"stage_to_local\": True,\n",
    "        \"local_dir\": \"/local_disk0/raster_cache\",\n",
    "        \"spark_tmp_dir\": \"/tmp/job3_grid_tmp\"\n",
    "    },\n",
    "    \"flags\": {\n",
    "        \"dry_run\": False,\n",
    "        \"preview\": True,\n",
    "        \"preview_rows\": 5,\n",
    "        \"overwrite_schema\": True,\n",
    "        \"write_mode\": \"overwrite\",\n",
    "        \"csv_infer_schema\": True,\n",
    "        \"save_temp_csv\": False,\n",
    "        \"save_per_tile\": False\n",
    "    },\n",
    "    \"workspace_base\": WORKSPACE_BASE\n",
    "}\n",
    "\n",
    "# Save minimal config to temp location\n",
    "temp_config_path = f\"/tmp/minimal_config_{ISO3}.yaml\"\n",
    "with open(temp_config_path, 'w') as f:\n",
    "    yaml.dump(minimal_config, f)\n",
    "\n",
    "# Upload to workspace\n",
    "workspace_config_path = f\"{WORKSPACE_BASE}/temp_minimal_config_{ISO3}.yaml\"\n",
    "dbutils.fs.cp(f\"file:{temp_config_path}\", f\"file:{workspace_config_path}\", recurse=True)\n",
    "\n",
    "print(f\"‚úÖ Minimal config created\")\n",
    "print(f\"üìç Location: {workspace_config_path}\")\n",
    "print(f\"\\nThis will be used by Task 0 to generate full config.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create Databricks Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"üî® Creating Databricks job...\")\n\njob_name = f\"Building_Enrichment_{ISO3}\"\nrequirements_path = f\"{WORKSPACE_BASE}/requirements.txt\"\n\n# Config path that Task 0 will generate\ngenerated_config_path = f\"{VOLUME_BASE}/{ISO3}/config.json\"\n\n# Define all tasks\ntasks = [\n    # Task 0: Setup & config generation\n    Task(\n        task_key=\"task0_setup\",\n        existing_cluster_id=CLUSTER_ID,\n        spark_python_task=SparkPythonTask(\n            python_file=f\"{WORKSPACE_BASE}/task0_setup.py\",\n            parameters=[\"--minimal_config\", workspace_config_path]\n        ),\n        libraries=[\n            Library(requirements=requirements_path)\n        ]\n    ),\n    # Task 1-7: Pipeline tasks (use config from Task 0)\n    Task(\n        task_key=\"task1_proportions_to_delta\",\n        depends_on=[TaskDependency(task_key=\"task0_setup\")],\n        existing_cluster_id=CLUSTER_ID,\n        spark_python_task=SparkPythonTask(\n            python_file=f\"{WORKSPACE_BASE}/task1_proportions_to_delta.py\",\n            parameters=[\"--config_path\", generated_config_path]\n        ),\n        libraries=[Library(requirements=requirements_path)]\n    ),\n    Task(\n        task_key=\"task2_grid_generation\",\n        depends_on=[TaskDependency(task_key=\"task1_proportions_to_delta\")],\n        existing_cluster_id=CLUSTER_ID,\n        spark_python_task=SparkPythonTask(\n            python_file=f\"{WORKSPACE_BASE}/task2_grid_generation.py\",\n            parameters=[\"--config_path\", generated_config_path]\n        ),\n        libraries=[Library(requirements=requirements_path)]\n    ),\n    Task(\n        task_key=\"task3_tile_downloader\",\n        depends_on=[TaskDependency(task_key=\"task2_grid_generation\")],\n        existing_cluster_id=CLUSTER_ID,\n        spark_python_task=SparkPythonTask(\n            python_file=f\"{WORKSPACE_BASE}/task3_tile_downloader.py\",\n            parameters=[\"--config_path\", generated_config_path]\n        ),\n        libraries=[Library(requirements=requirements_path)]\n    ),\n    Task(\n        task_key=\"task4_raster_stats\",\n        depends_on=[TaskDependency(task_key=\"task3_tile_downloader\")],\n        existing_cluster_id=CLUSTER_ID,\n        spark_python_task=SparkPythonTask(\n            python_file=f\"{WORKSPACE_BASE}/task4_raster_stats.py\",\n            parameters=[\"--config_path\", generated_config_path]\n        ),\n        libraries=[Library(requirements=requirements_path)]\n    ),\n    Task(\n        task_key=\"task5_post_processing\",\n        depends_on=[TaskDependency(task_key=\"task4_raster_stats\")],\n        existing_cluster_id=CLUSTER_ID,\n        spark_python_task=SparkPythonTask(\n            python_file=f\"{WORKSPACE_BASE}/task5_post_processing.py\",\n            parameters=[\"--config_path\", generated_config_path]\n        ),\n        libraries=[Library(requirements=requirements_path)]\n    ),\n    Task(\n        task_key=\"task6_create_views\",\n        depends_on=[TaskDependency(task_key=\"task5_post_processing\")],\n        existing_cluster_id=CLUSTER_ID,\n        spark_python_task=SparkPythonTask(\n            python_file=f\"{WORKSPACE_BASE}/task6_create_views.py\",\n            parameters=[\"--config_path\", generated_config_path]\n        ),\n        libraries=[Library(requirements=requirements_path)]\n    ),\n    Task(\n        task_key=\"task7_export\",\n        depends_on=[TaskDependency(task_key=\"task6_create_views\")],\n        existing_cluster_id=CLUSTER_ID,\n        spark_python_task=SparkPythonTask(\n            python_file=f\"{WORKSPACE_BASE}/task7_export.py\",\n            parameters=[\"--config_path\", generated_config_path, \"--iso3\", ISO3]\n        ),\n        libraries=[Library(requirements=requirements_path)]\n    )\n]\n\n# Create job\njob = w.jobs.create(\n    name=job_name,\n    tasks=tasks,\n    max_concurrent_runs=1,\n    timeout_seconds=0,\n    email_notifications={\n        \"on_success\": [EMAIL],\n        \"on_failure\": [EMAIL]\n    } if EMAIL else None\n)\n\nJOB_ID = job.job_id\nprint(f\"‚úÖ Job created successfully!\")\nprint(f\"   Job ID: {JOB_ID}\")\nprint(f\"   Job Name: {job_name}\")\nprint(f\"   Tasks: {len(tasks)} (including Task 0 setup)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Run Job & Monitor Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"üöÄ Starting job {JOB_ID}...\")\n",
    "\n",
    "# Run the job\n",
    "run = w.jobs.run_now(job_id=JOB_ID)\n",
    "RUN_ID = run.run_id\n",
    "\n",
    "print(f\"‚úÖ Job started!\")\n",
    "print(f\"   Run ID: {RUN_ID}\")\n",
    "print(f\"\")\n",
    "print(f\"‚è≥ Monitoring job progress...\")\n",
    "print(f\"   (Updates every 30 seconds)\")\n",
    "print(f\"\")\n",
    "\n",
    "# Monitor job progress\n",
    "start_time = time.time()\n",
    "last_state = None\n",
    "last_task_status = {}\n",
    "\n",
    "while True:\n",
    "    run_info = w.jobs.get_run(run_id=RUN_ID)\n",
    "    state = run_info.state\n",
    "    life_cycle_state = state.life_cycle_state.value\n",
    "    \n",
    "    # Print state changes\n",
    "    if life_cycle_state != last_state:\n",
    "        elapsed = int(time.time() - start_time)\n",
    "        print(f\"[{elapsed}s] Job status: {life_cycle_state}\")\n",
    "        last_state = life_cycle_state\n",
    "    \n",
    "    # Print task progress\n",
    "    if run_info.tasks:\n",
    "        for task in run_info.tasks:\n",
    "            task_key = task.task_key\n",
    "            task_state = task.state.life_cycle_state.value if task.state else \"PENDING\"\n",
    "            \n",
    "            if task_key not in last_task_status or last_task_status[task_key] != task_state:\n",
    "                elapsed = int(time.time() - start_time)\n",
    "                status_icon = \"‚è≥\" if task_state == \"RUNNING\" else \"‚úÖ\" if task_state == \"TERMINATED\" else \"‚è∏Ô∏è\"\n",
    "                print(f\"[{elapsed}s] {status_icon} {task_key}: {task_state}\")\n",
    "                last_task_status[task_key] = task_state\n",
    "    \n",
    "    # Check if job is done\n",
    "    if life_cycle_state in [\"TERMINATED\", \"INTERNAL_ERROR\", \"SKIPPED\"]:\n",
    "        result_state = state.result_state.value if state.result_state else \"UNKNOWN\"\n",
    "        elapsed = int(time.time() - start_time)\n",
    "        \n",
    "        if result_state == \"SUCCESS\":\n",
    "            print(f\"\")\n",
    "            print(f\"‚úÖ Job completed successfully!\")\n",
    "            print(f\"   Duration: {elapsed // 60}m {elapsed % 60}s\")\n",
    "        else:\n",
    "            print(f\"\")\n",
    "            print(f\"‚ùå Job failed with state: {result_state}\")\n",
    "            print(f\"   Duration: {elapsed // 60}m {elapsed % 60}s\")\n",
    "            if state.state_message:\n",
    "                print(f\"   Error: {state.state_message}\")\n",
    "        break\n",
    "    \n",
    "    # Wait before next check\n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Verify Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç Verifying outputs...\\n\")\n",
    "\n",
    "# Check main output table\n",
    "output_table = f\"{CATALOG}.{SCHEMA}.building_enrichment_output_{ISO3}\"\n",
    "\n",
    "try:\n",
    "    df = spark.table(output_table)\n",
    "    count = df.count()\n",
    "    print(f\"‚úÖ Main output table exists: {output_table}\")\n",
    "    print(f\"   Row count: {count:,}\\n\")\n",
    "    print(f\"   Sample data:\")\n",
    "    display(df.limit(5))\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Could not verify table: {e}\")\n",
    "\n",
    "print(f\"\\nüìä Export files location: {VOLUME_BASE}/{ISO3}/output/exports/FULL_{ISO3}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"PIPELINE EXECUTION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Country: {ISO3}\")\n",
    "print(f\"Job ID: {JOB_ID}\")\n",
    "print(f\"Run ID: {RUN_ID}\")\n",
    "print(f\"\")\n",
    "print(f\"üìÅ Data Location: {VOLUME_BASE}/{ISO3}\")\n",
    "print(f\"üìä Main Output Table: {output_table}\")\n",
    "print(f\"üìÇ Exports: {VOLUME_BASE}/{ISO3}/output/exports/FULL_{ISO3}/\")\n",
    "print(f\"‚öôÔ∏è  Config: {generated_config_path}\")\n",
    "print(f\"\")\n",
    "print(f\"View job in Databricks UI: Workflows ‚Üí Jobs ‚Üí {job_name}\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}