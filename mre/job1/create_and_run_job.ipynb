{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Enrichment Pipeline - Job Creator & Runner\n",
    "\n",
    "This notebook orchestrates the entire building enrichment pipeline:\n",
    "1. Auto-installs required packages\n",
    "2. Creates ISO3-based folder structure\n",
    "3. Copies input files to the correct locations\n",
    "4. Generates job configuration dynamically\n",
    "5. Creates and submits Databricks job\n",
    "6. Monitors job progress in real-time\n",
    "7. Exports final outputs\n",
    "\n",
    "**Prerequisites:**\n",
    "- Edit `job_config.yaml` with your settings\n",
    "- Ensure input files exist at specified paths\n",
    "- Run this notebook on a cluster with Databricks Runtime 13.3+ LTS\n",
    "\n",
    "**No CLI required** - Everything runs via Databricks SDK and workspace APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Notebook Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages needed for THIS notebook (not the job itself)\n",
    "print(\"üì¶ Checking notebook dependencies...\")\n",
    "\n",
    "try:\n",
    "    import databricks.sdk\n",
    "    import yaml\n",
    "    print(\"‚úÖ All notebook dependencies available\")\nexcept ImportError:\n",
    "    print(\"‚öôÔ∏è  Installing missing packages...\")\n",
    "    %pip install databricks-sdk pyyaml --quiet\n",
    "    print(\"‚úÖ Packages installed successfully\")\n",
    "    print(\"üîÑ Restarting Python kernel...\")\n",
    "    dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Import Libraries & Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import yaml\n",
    "\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.jobs import (\n",
    "    Task, NotebookTask, Source, JobCluster, ClusterSpec,\n",
    "    AutoScale, RuntimeEngine, JobSettings, TaskDependency\n",
    ")\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Initialize Databricks Workspace Client\n",
    "w = WorkspaceClient()\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get current notebook's directory\n",
    "notebook_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "notebook_dir = str(Path(notebook_path).parent)\n",
    "\n",
    "# Load job configuration\n",
    "config_path = f\"/Workspace{notebook_dir}/job_config.yaml\"\n",
    "print(f\"üìÑ Loading config from: {config_path}\")\n",
    "\n",
    "with open(config_path.replace('/Workspace', '/Workspace'), 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Extract key values\n",
    "ISO3 = config['iso3']\n",
    "CATALOG = config['databricks']['catalog']\n",
    "SCHEMA = config['databricks']['schema']\n",
    "WORKSPACE_BASE = config['databricks']['workspace_base']\n",
    "VOLUME_BASE = config['databricks']['volume_base']\n",
    "\n",
    "print(f\"‚úÖ Configuration loaded for country: {ISO3}\")\n",
    "print(f\"   Catalog: {CATALOG}\")\n",
    "print(f\"   Schema: {SCHEMA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Auto-Detect or Get Cluster ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-detect current cluster or use specified one\n",
    "if config['databricks'].get('cluster_id'):\n",
    "    CLUSTER_ID = config['databricks']['cluster_id']\n",
    "    print(f\"üìå Using specified cluster ID: {CLUSTER_ID}\")\n",
    "else:\n",
    "    CLUSTER_ID = spark.conf.get(\"spark.databricks.clusterUsageTags.clusterId\")\n",
    "    print(f\"üîç Auto-detected current cluster ID: {CLUSTER_ID}\")\n",
    "\n",
    "# Get cluster details\n",
    "try:\n",
    "    cluster_info = w.clusters.get(cluster_id=CLUSTER_ID)\n",
    "    print(f\"‚úÖ Cluster found: {cluster_info.cluster_name}\")\n",
    "    print(f\"   Runtime: {cluster_info.spark_version}\")\n",
    "    print(f\"   State: {cluster_info.state}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Warning: Could not fetch cluster details: {e}\")\n",
    "    print(f\"   Will still use cluster ID: {CLUSTER_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create Folder Structure with ISO3 First"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"üìÅ Creating folder structure for {ISO3}...\")\n",
    "\n",
    "# Define folder structure: {ISO3}/input/, {ISO3}/output/, {ISO3}/logs/\n",
    "BASE_DATA_DIR = f\"{VOLUME_BASE}/{ISO3}\"\n",
    "INPUT_DIR = f\"{BASE_DATA_DIR}/input\"\n",
    "TILES_DIR = f\"{INPUT_DIR}/tiles\"\n",
    "OUTPUT_DIR = f\"{BASE_DATA_DIR}/output\"\n",
    "LOGS_DIR = f\"{BASE_DATA_DIR}/logs\"\n",
    "\n",
    "# Create directories\n",
    "dbutils.fs.mkdirs(BASE_DATA_DIR)\n",
    "dbutils.fs.mkdirs(INPUT_DIR)\n",
    "dbutils.fs.mkdirs(TILES_DIR)\n",
    "dbutils.fs.mkdirs(OUTPUT_DIR)\n",
    "dbutils.fs.mkdirs(LOGS_DIR)\n",
    "\n",
    "print(f\"‚úÖ Folder structure created:\")\n",
    "print(f\"   Base: {BASE_DATA_DIR}\")\n",
    "print(f\"   Input: {INPUT_DIR}\")\n",
    "print(f\"   Tiles: {TILES_DIR}\")\n",
    "print(f\"   Output: {OUTPUT_DIR}\")\n",
    "print(f\"   Logs: {LOGS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Copy Input Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìÇ Copying input files...\")\n",
    "\n",
    "# Copy tile shapefile from git data/ folder to {ISO3}/input/tiles/\n",
    "TILES_SOURCE = f\"{WORKSPACE_BASE}/ghsl2_0_mwd_l1_tile_schema_land.gpkg\"\n",
    "TILES_DEST = f\"{TILES_DIR}/ghsl2_0_mwd_l1_tile_schema_land.gpkg\"\n",
    "\n",
    "print(f\"   Copying tiles: {TILES_SOURCE} -> {TILES_DEST}\")\n",
    "dbutils.fs.cp(TILES_SOURCE.replace('/Workspace', 'file:/Workspace'), TILES_DEST, recurse=True)\n",
    "print(f\"   ‚úÖ Tiles copied\")\n",
    "\n",
    "# Copy TSI CSV\n",
    "tsi_source = config['inputs']['tsi_csv']\n",
    "tsi_dest = f\"{INPUT_DIR}/tsi.csv\"\n",
    "print(f\"   Copying TSI: {tsi_source} -> {tsi_dest}\")\n",
    "dbutils.fs.cp(tsi_source, tsi_dest, recurse=True)\n",
    "print(f\"   ‚úÖ TSI copied\")\n",
    "\n",
    "# Copy Proportions CSV\n",
    "proportions_source = config['inputs']['proportions_csv']\n",
    "proportions_dest = f\"{INPUT_DIR}/proportions.csv\"\n",
    "print(f\"   Copying Proportions: {proportions_source} -> {proportions_dest}\")\n",
    "dbutils.fs.cp(proportions_source, proportions_dest, recurse=True)\n",
    "print(f\"   ‚úÖ Proportions copied\")\n",
    "\n",
    "# Copy World boundaries if provided\n",
    "if config['inputs'].get('world_boundaries'):\n",
    "    world_source = config['inputs']['world_boundaries']\n",
    "    world_dest = f\"{INPUT_DIR}/world_boundaries.gpkg\"\n",
    "    print(f\"   Copying World boundaries: {world_source} -> {world_dest}\")\n",
    "    dbutils.fs.cp(world_source, world_dest, recurse=True)\n",
    "    print(f\"   ‚úÖ World boundaries copied\")\n",
    "\n",
    "print(\"‚úÖ All input files copied successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Generate Config with ISO3 Suffix in Table Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"‚öôÔ∏è  Generating pipeline configuration...\")\n\n# Helper function to create table names with ISO3 suffix\ndef table_name(base_name):\n    return f\"{CATALOG}.{SCHEMA}.{base_name}_{ISO3}\"\n\n# Create full pipeline config with ISO3 suffix in ALL table names\npipeline_config = {\n    # Core settings\n    \"catalog\": CATALOG,\n    \"schema\": SCHEMA,\n    \"iso3\": ISO3,\n\n    # Input paths\n    \"proportions_csv_path\": proportions_dest,\n    \"tsi_csv_path\": tsi_dest,\n    \"admin_path\": config['inputs'].get('world_boundaries', world_dest if 'world_dest' in locals() else ''),\n    \"tile_footprint_path\": TILES_DEST,\n\n    # Delta tables with ISO3 suffix\n    \"proportions_path\": table_name(\"building_enrichment_proportions_input\"),\n    \"proportions_table\": table_name(\"building_enrichment_proportions_input\"),\n    \"tsi_table\": table_name(\"building_enrichment_tsi_input\"),\n    \"delta_table_base\": table_name(\"grid_centroids\"),\n    \"grid_source\": table_name(\"grid_centroids\"),\n    \"grid_count_table\": table_name(\"grid_counts\"),\n    \"counts_delta_table\": table_name(\"grid_counts\"),\n    \"output_table\": table_name(\"building_enrichment_output\"),\n    \"download_status_table\": table_name(\"download_status\"),\n\n    # Output directories\n    \"output_dir\": OUTPUT_DIR,\n    \"grid_output_csv\": f\"{OUTPUT_DIR}/grid_centroids.csv\",\n    \"tiles_dest_root\": f\"{INPUT_DIR}/tiles\",\n    \"built_root\": f\"{INPUT_DIR}/tiles/built_c\",\n    \"smod_root\": f\"{INPUT_DIR}/tiles/smod\",\n\n    # Admin boundaries\n    \"admin_field\": \"ISO3\",\n    \"admin_value\": ISO3,\n\n    # Tile footprints\n    \"tile_id_field\": \"tile_id\",\n\n    # Processing parameters\n    \"cell_size\": config.get('params', {}).get('cell_size', 2000),\n    \"export_crs\": \"EPSG:4326\",\n    \"target_crs\": \"ESRI:54009\",\n    \"datasets\": \"built_c,smod\",\n    \"download_concurrency\": config.get('params', {}).get('download_concurrency', 3),\n    \"download_retries\": 2,\n    \"use_smod\": True,\n    \"use_boundary_mask\": True,\n    \"include_nodata\": True,\n    \"add_percentages\": False,\n    \"chunk_size\": 10000,\n    \"max_workers\": config.get('params', {}).get('max_workers', 8),\n    \"tile_parallelism\": str(config.get('params', {}).get('tile_parallelism', 4)),\n    \"SAMPLE_SIZE\": 10000,\n    \"stage_to_local\": True,\n    \"local_dir\": \"/local_disk0/raster_cache\",\n    \"spark_tmp_dir\": \"/tmp/job3_grid_tmp\",\n\n    # Flags\n    \"dry_run\": False,\n    \"preview\": True,\n    \"preview_rows\": 5,\n    \"overwrite_schema\": True,\n    \"write_mode\": \"overwrite\",\n    \"csv_infer_schema\": True,\n    \"save_temp_csv\": False,\n    \"save_per_tile\": False\n}\n\n# Save config as JSON for tasks to use\nconfig_json_path = f\"{BASE_DATA_DIR}/config.json\"\nconfig_json_local = config_json_path.replace('dbfs:', '/dbfs')\n\nwith open(config_json_local, 'w') as f:\n    json.dump(pipeline_config, f, indent=2)\n\nprint(f\"‚úÖ Configuration saved to: {config_json_path}\")\nprint(f\"   All table names will include suffix: _{ISO3}\")\nprint(f\"   Output table: {pipeline_config['output_table']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Create Databricks Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üî® Creating Databricks job...\")\n",
    "\n",
    "# Define job name with ISO3\n",
    "job_name = config.get('job', {}).get('name', 'Building_Enrichment_{ISO3}').replace('{ISO3}', ISO3)\n",
    "\n",
    "# Requirements.txt path\n",
    "requirements_path = f\"{WORKSPACE_BASE}/requirements.txt\"\n",
    "\n",
    "# Define tasks\n",
    "tasks = [\n",
    "    Task(\n",
    "        task_key=\"task1_proportions_to_delta\",\n",
    "        existing_cluster_id=CLUSTER_ID,\n",
    "        python_wheel_task=None,\n",
    "        spark_python_task={\n",
    "            \"python_file\": f\"{WORKSPACE_BASE}/task1_proportions_to_delta.py\",\n",
    "            \"parameters\": [\"--config_path\", config_json_path]\n",
    "        },\n",
    "        libraries=[\n",
    "            {\"requirements\": requirements_path}\n",
    "        ]\n",
    "    ),\n",
    "    Task(\n",
    "        task_key=\"task2_grid_generation\",\n",
    "        depends_on=[TaskDependency(task_key=\"task1_proportions_to_delta\")],\n",
    "        existing_cluster_id=CLUSTER_ID,\n",
    "        spark_python_task={\n",
    "            \"python_file\": f\"{WORKSPACE_BASE}/task2_grid_generation.py\",\n",
    "            \"parameters\": [\"--config_path\", config_json_path]\n",
    "        },\n",
    "        libraries=[\n",
    "            {\"requirements\": requirements_path}\n",
    "        ]\n",
    "    ),\n",
    "    Task(\n",
    "        task_key=\"task3_tile_downloader\",\n",
    "        depends_on=[TaskDependency(task_key=\"task2_grid_generation\")],\n",
    "        existing_cluster_id=CLUSTER_ID,\n",
    "        spark_python_task={\n",
    "            \"python_file\": f\"{WORKSPACE_BASE}/task3_tile_downloader.py\",\n",
    "            \"parameters\": [\"--config_path\", config_json_path]\n",
    "        },\n",
    "        libraries=[\n",
    "            {\"requirements\": requirements_path}\n",
    "        ]\n",
    "    ),\n",
    "    Task(\n",
    "        task_key=\"task4_raster_stats\",\n",
    "        depends_on=[TaskDependency(task_key=\"task3_tile_downloader\")],\n",
    "        existing_cluster_id=CLUSTER_ID,\n",
    "        spark_python_task={\n",
    "            \"python_file\": f\"{WORKSPACE_BASE}/task4_raster_stats.py\",\n",
    "            \"parameters\": [\"--config_path\", config_json_path]\n",
    "        },\n",
    "        libraries=[\n",
    "            {\"requirements\": requirements_path}\n",
    "        ]\n",
    "    ),\n",
    "    Task(\n",
    "        task_key=\"task5_post_processing\",\n",
    "        depends_on=[TaskDependency(task_key=\"task4_raster_stats\")],\n",
    "        existing_cluster_id=CLUSTER_ID,\n",
    "        spark_python_task={\n",
    "            \"python_file\": f\"{WORKSPACE_BASE}/task5_post_processing.py\",\n",
    "            \"parameters\": [\"--config_path\", config_json_path]\n",
    "        },\n",
    "        libraries=[\n",
    "            {\"requirements\": requirements_path}\n",
    "        ]\n",
    "    ),\n",
    "    Task(\n",
    "        task_key=\"task6_create_views\",\n",
    "        depends_on=[TaskDependency(task_key=\"task5_post_processing\")],\n",
    "        existing_cluster_id=CLUSTER_ID,\n",
    "        spark_python_task={\n",
    "            \"python_file\": f\"{WORKSPACE_BASE}/task6_create_views.py\",\n",
    "            \"parameters\": [\"--config_path\", config_json_path]\n",
    "        },\n",
    "        libraries=[\n",
    "            {\"requirements\": requirements_path}\n",
    "        ]\n",
    "    ),\n",
    "    Task(\n",
    "        task_key=\"task7_export\",\n",
    "        depends_on=[TaskDependency(task_key=\"task6_create_views\")],\n",
    "        existing_cluster_id=CLUSTER_ID,\n",
    "        spark_python_task={\n",
    "            \"python_file\": f\"{WORKSPACE_BASE}/task7_export.py\",\n",
    "            \"parameters\": [\"--config_path\", config_json_path, \"--iso3\", ISO3]\n",
    "        },\n",
    "        libraries=[\n",
    "            {\"requirements\": requirements_path}\n",
    "        ]\n",
    "    )\n",
    "]\n",
    "\n",
    "# Create job\n",
    "job = w.jobs.create(\n",
    "    name=job_name,\n",
    "    tasks=tasks,\n",
    "    max_concurrent_runs=config.get('job', {}).get('max_concurrent_runs', 1),\n",
    "    timeout_seconds=config.get('job', {}).get('timeout_seconds', 0),\n",
    "    email_notifications={\n",
    "        \"on_success\": [config.get('job', {}).get('email_notifications', '')],\n",
    "        \"on_failure\": [config.get('job', {}).get('email_notifications', '')]\n",
    "    } if config.get('job', {}).get('email_notifications') else None\n",
    ")\n",
    "\n",
    "JOB_ID = job.job_id\n",
    "print(f\"‚úÖ Job created successfully!\")\n",
    "print(f\"   Job ID: {JOB_ID}\")\n",
    "print(f\"   Job Name: {job_name}\")\n",
    "print(f\"   Tasks: {len(tasks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Run Job & Monitor Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"üöÄ Starting job {JOB_ID}...\")\n",
    "\n",
    "# Run the job\n",
    "run = w.jobs.run_now(job_id=JOB_ID)\n",
    "RUN_ID = run.run_id\n",
    "\n",
    "print(f\"‚úÖ Job started!\")\n",
    "print(f\"   Run ID: {RUN_ID}\")\n",
    "print(f\"\")\n",
    "print(f\"‚è≥ Monitoring job progress...\")\n",
    "print(f\"   (This will update every 30 seconds until completion)\")\n",
    "print(f\"\")\n",
    "\n",
    "# Monitor job progress\n",
    "start_time = time.time()\n",
    "last_state = None\n",
    "last_task_status = {}\n",
    "\n",
    "while True:\n",
    "    run_info = w.jobs.get_run(run_id=RUN_ID)\n",
    "    state = run_info.state\n",
    "    life_cycle_state = state.life_cycle_state.value\n",
    "    \n",
    "    # Print state changes\n",
    "    if life_cycle_state != last_state:\n",
    "        elapsed = int(time.time() - start_time)\n",
    "        print(f\"[{elapsed}s] Job status: {life_cycle_state}\")\n",
    "        last_state = life_cycle_state\n",
    "    \n",
    "    # Print task progress\n",
    "    if run_info.tasks:\n",
    "        for task in run_info.tasks:\n",
    "            task_key = task.task_key\n",
    "            task_state = task.state.life_cycle_state.value if task.state else \"PENDING\"\n",
    "            \n",
    "            if task_key not in last_task_status or last_task_status[task_key] != task_state:\n",
    "                elapsed = int(time.time() - start_time)\n",
    "                status_icon = \"‚è≥\" if task_state == \"RUNNING\" else \"‚úÖ\" if task_state == \"TERMINATED\" else \"‚è∏Ô∏è\"\n",
    "                print(f\"[{elapsed}s] {status_icon} {task_key}: {task_state}\")\n",
    "                last_task_status[task_key] = task_state\n",
    "    \n",
    "    # Check if job is done\n",
    "    if life_cycle_state in [\"TERMINATED\", \"INTERNAL_ERROR\", \"SKIPPED\"]:\n",
    "        result_state = state.result_state.value if state.result_state else \"UNKNOWN\"\n",
    "        elapsed = int(time.time() - start_time)\n",
    "        \n",
    "        if result_state == \"SUCCESS\":\n",
    "            print(f\"\")\n",
    "            print(f\"‚úÖ Job completed successfully!\")\n",
    "            print(f\"   Duration: {elapsed // 60}m {elapsed % 60}s\")\n",
    "        else:\n",
    "            print(f\"\")\n",
    "            print(f\"‚ùå Job failed with state: {result_state}\")\n",
    "            print(f\"   Duration: {elapsed // 60}m {elapsed % 60}s\")\n",
    "            if state.state_message:\n",
    "                print(f\"   Error: {state.state_message}\")\n",
    "        break\n",
    "    \n",
    "    # Wait before next check\n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Verify Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç Verifying outputs...\")\n",
    "print(f\"\")\n",
    "\n",
    "# Check if main output table exists\n",
    "output_table = f\"{CATALOG}.{SCHEMA}.building_enrichment_output_{ISO3}\"\n",
    "\n",
    "try:\n",
    "    df = spark.table(output_table)\n",
    "    count = df.count()\n",
    "    print(f\"‚úÖ Main output table exists: {output_table}\")\n",
    "    print(f\"   Row count: {count:,}\")\n",
    "    print(f\"\")\n",
    "    print(f\"   Sample data:\")\n",
    "    display(df.limit(5))\nexcept Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Warning: Could not verify table {output_table}\")\n",
    "    print(f\"   Error: {e}\")\n",
    "\n",
    "print(f\"\")\n",
    "print(f\"üìä Check export files in: {OUTPUT_DIR}/exports/FULL_{ISO3}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"PIPELINE EXECUTION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Country: {ISO3}\")\n",
    "print(f\"Job ID: {JOB_ID}\")\n",
    "print(f\"Run ID: {RUN_ID}\")\n",
    "print(f\"\")\n",
    "print(f\"üìÅ Data Location: {BASE_DATA_DIR}\")\n",
    "print(f\"üìä Output Table: {output_table}\")\n",
    "print(f\"üìÇ Export Location: {OUTPUT_DIR}/exports/FULL_{ISO3}/\")\n",
    "print(f\"\")\n",
    "print(f\"To view job details in Databricks UI:\")\n",
    "print(f\"Workflows ‚Üí Jobs ‚Üí {job_name}\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}